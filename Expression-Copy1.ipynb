{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('../face_recognition/haarcascade_frontalface_default.xml')\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH=\"../facial_expressions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions=[\"anger\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(IMG_PATH+\"icml_face_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace(to_replace=\"PrivateTest\",value=\"Testing\")\n",
    "df=df.replace(to_replace=\"PublicTest\",value=\"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>Usage</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Training</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Training</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Training</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Training</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>Testing</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>Testing</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>Testing</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>Testing</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>Testing</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35887 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion     Usage                                             pixels\n",
       "0            0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
       "1            0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
       "2            2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
       "3            4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
       "4            6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
       "...        ...       ...                                                ...\n",
       "35882        6   Testing  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...\n",
       "35883        3   Testing  178 174 172 173 181 188 191 194 196 199 200 20...\n",
       "35884        0   Testing  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...\n",
       "35885        3   Testing  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...\n",
       "35886        2   Testing  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...\n",
       "\n",
       "[35887 rows x 3 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"Training\":[],\"Testing\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35887  4965      32797  "
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(\"\\r\",i+1,end=\" \")\n",
    "    row=df.iloc[i]\n",
    "    img=np.array(list(map(np.uint8,row[2].split()))).reshape(48,48)\n",
    "    data[row[1]].append([row[0],ret_keypoints(img)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dim=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_keypoints(image):\n",
    "    gray=image\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.resize(gray, (face_dim, face_dim))\n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    param=[0,0,face_dim,face_dim]\n",
    "    shape=predictor(gray,dlib.rectangle(*param))\n",
    "    xlist=[]\n",
    "    ylist=[]\n",
    "    for i in range(68):\n",
    "        xlist.append(np.float32(shape.part(i).x))\n",
    "        ylist.append(np.float32(shape.part(i).y))\n",
    "    xmean = np.mean(xlist)\n",
    "    ymean = np.mean(ylist)\n",
    "#     plt.imshow(gray,cmap='gray')\n",
    "#     plt.scatter(xlist,ylist, marker='.')\n",
    "#     plt.show()\n",
    "    xcentral = [(x-xmean) for x in xlist]\n",
    "    ycentral = [(y-ymean) for y in ylist]\n",
    "    res=[]\n",
    "    for (x,y) in zip(xcentral,ycentral):\n",
    "        res.append(x)\n",
    "        res.append(y)\n",
    "    return (np.asarray(res)/face_dim+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=np.arange(len(XXD))\n",
    "# for i in range(5):\n",
    "#     np.random.shuffle(s)\n",
    "#     XXD=XXD[s]\n",
    "#     YYD=YYD[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(1024,activation='relu', input_shape=(68*2,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(emotions),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1024)              140288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 837,895\n",
      "Trainable params: 834,055\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "XDB=np.vstack(np.asarray(data[\"Training\"])[:,1])\n",
    "YDY=np.asarray(data[\"Training\"])[:,0]\n",
    "YDB=np.zeros((len(YDY),len(emotions)))\n",
    "for i in range(len(YDB)):\n",
    "    YDB[i][YDY[i]]=1\n",
    "XTB=np.vstack(np.asarray(data[\"Testing\"])[:,1])\n",
    "YTY=np.asarray(data[\"Testing\"])[:,0]\n",
    "YTB=np.zeros((len(YTY),len(emotions)))\n",
    "for i in range(len(YTB)):\n",
    "    YTB[i][YTY[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "32298/32298 [==============================] - 7s 231us/sample - loss: 1.2524 - accuracy: 0.5162 - val_loss: 1.4062 - val_accuracy: 0.4678\n",
      "Epoch 2/100\n",
      "32298/32298 [==============================] - 8s 257us/sample - loss: 1.2488 - accuracy: 0.5179 - val_loss: 1.3018 - val_accuracy: 0.5026\n",
      "Epoch 3/100\n",
      "32298/32298 [==============================] - 7s 226us/sample - loss: 1.2486 - accuracy: 0.5162 - val_loss: 1.3351 - val_accuracy: 0.4809\n",
      "Epoch 4/100\n",
      "32298/32298 [==============================] - 7s 220us/sample - loss: 1.2479 - accuracy: 0.5199 - val_loss: 1.2618 - val_accuracy: 0.5088\n",
      "Epoch 5/100\n",
      "32298/32298 [==============================] - 8s 247us/sample - loss: 1.2462 - accuracy: 0.5201 - val_loss: 1.3826 - val_accuracy: 0.4673\n",
      "Epoch 6/100\n",
      "32298/32298 [==============================] - 8s 236us/sample - loss: 1.2484 - accuracy: 0.5172 - val_loss: 1.2854 - val_accuracy: 0.4976\n",
      "Epoch 7/100\n",
      "32298/32298 [==============================] - 7s 208us/sample - loss: 1.2487 - accuracy: 0.5182 - val_loss: 1.5119 - val_accuracy: 0.4182\n",
      "Epoch 8/100\n",
      "32298/32298 [==============================] - 7s 223us/sample - loss: 1.2471 - accuracy: 0.5181 - val_loss: 1.3069 - val_accuracy: 0.4923\n",
      "Epoch 9/100\n",
      "32298/32298 [==============================] - 7s 215us/sample - loss: 1.2448 - accuracy: 0.5178 - val_loss: 1.3112 - val_accuracy: 0.4976\n",
      "Epoch 10/100\n",
      "32298/32298 [==============================] - 8s 237us/sample - loss: 1.2457 - accuracy: 0.5200 - val_loss: 1.3221 - val_accuracy: 0.4773\n",
      "Epoch 11/100\n",
      "32298/32298 [==============================] - 7s 217us/sample - loss: 1.2455 - accuracy: 0.5180 - val_loss: 1.3876 - val_accuracy: 0.4767\n",
      "Epoch 12/100\n",
      "32298/32298 [==============================] - 7s 224us/sample - loss: 1.2406 - accuracy: 0.5192 - val_loss: 1.4323 - val_accuracy: 0.4397\n",
      "Epoch 13/100\n",
      "32298/32298 [==============================] - 9s 279us/sample - loss: 1.2420 - accuracy: 0.5211 - val_loss: 1.3402 - val_accuracy: 0.4817\n",
      "Epoch 14/100\n",
      "32298/32298 [==============================] - 8s 242us/sample - loss: 1.2411 - accuracy: 0.5214 - val_loss: 1.3704 - val_accuracy: 0.4628\n",
      "Epoch 15/100\n",
      "32298/32298 [==============================] - 9s 270us/sample - loss: 1.2385 - accuracy: 0.5221 - val_loss: 1.3275 - val_accuracy: 0.4915\n",
      "Epoch 16/100\n",
      "32298/32298 [==============================] - 8s 249us/sample - loss: 1.2394 - accuracy: 0.5204 - val_loss: 1.4141 - val_accuracy: 0.4826\n",
      "Epoch 17/100\n",
      "32298/32298 [==============================] - 7s 225us/sample - loss: 1.2415 - accuracy: 0.5180 - val_loss: 1.3311 - val_accuracy: 0.4784\n",
      "Epoch 18/100\n",
      "32298/32298 [==============================] - 7s 224us/sample - loss: 1.2406 - accuracy: 0.5209 - val_loss: 1.5224 - val_accuracy: 0.4048\n",
      "Epoch 19/100\n",
      "32298/32298 [==============================] - 7s 231us/sample - loss: 1.2421 - accuracy: 0.5222 - val_loss: 1.4289 - val_accuracy: 0.4400\n",
      "Epoch 20/100\n",
      "32298/32298 [==============================] - 7s 223us/sample - loss: 1.2364 - accuracy: 0.5214 - val_loss: 1.3206 - val_accuracy: 0.4870\n",
      "Epoch 21/100\n",
      "32298/32298 [==============================] - 7s 227us/sample - loss: 1.2404 - accuracy: 0.5215 - val_loss: 1.2727 - val_accuracy: 0.5079\n",
      "Epoch 22/100\n",
      "32298/32298 [==============================] - 8s 247us/sample - loss: 1.2404 - accuracy: 0.5182 - val_loss: 1.2876 - val_accuracy: 0.5074\n",
      "Epoch 23/100\n",
      "32298/32298 [==============================] - 7s 225us/sample - loss: 1.2376 - accuracy: 0.5220 - val_loss: 1.3147 - val_accuracy: 0.4904\n",
      "Epoch 24/100\n",
      "32298/32298 [==============================] - 8s 250us/sample - loss: 1.2356 - accuracy: 0.5207 - val_loss: 1.2913 - val_accuracy: 0.4901\n",
      "Epoch 25/100\n",
      "32298/32298 [==============================] - 8s 261us/sample - loss: 1.2330 - accuracy: 0.5225 - val_loss: 1.4493 - val_accuracy: 0.4480\n",
      "Epoch 26/100\n",
      "32298/32298 [==============================] - 7s 221us/sample - loss: 1.2433 - accuracy: 0.5175 - val_loss: 1.2694 - val_accuracy: 0.5057\n",
      "Epoch 27/100\n",
      "32298/32298 [==============================] - 8s 244us/sample - loss: 1.2312 - accuracy: 0.5250 - val_loss: 1.4063 - val_accuracy: 0.4505\n",
      "Epoch 28/100\n",
      "32298/32298 [==============================] - 8s 252us/sample - loss: 1.2381 - accuracy: 0.5204 - val_loss: 1.3478 - val_accuracy: 0.4817\n",
      "Epoch 29/100\n",
      "32298/32298 [==============================] - 10s 300us/sample - loss: 1.2345 - accuracy: 0.5237 - val_loss: 1.2679 - val_accuracy: 0.5102\n",
      "Epoch 30/100\n",
      "32298/32298 [==============================] - 8s 255us/sample - loss: 1.2374 - accuracy: 0.5230 - val_loss: 1.3080 - val_accuracy: 0.4848\n",
      "Epoch 31/100\n",
      "32298/32298 [==============================] - 7s 203us/sample - loss: 1.2362 - accuracy: 0.5226 - val_loss: 1.3033 - val_accuracy: 0.5007\n",
      "Epoch 32/100\n",
      "32298/32298 [==============================] - 7s 226us/sample - loss: 1.2349 - accuracy: 0.5235 - val_loss: 1.3298 - val_accuracy: 0.4790\n",
      "Epoch 33/100\n",
      "32298/32298 [==============================] - 6s 189us/sample - loss: 1.2376 - accuracy: 0.5202 - val_loss: 1.5388 - val_accuracy: 0.4082\n",
      "Epoch 34/100\n",
      "32298/32298 [==============================] - 6s 185us/sample - loss: 1.2330 - accuracy: 0.5233 - val_loss: 1.3102 - val_accuracy: 0.4907\n",
      "Epoch 35/100\n",
      "32298/32298 [==============================] - 6s 181us/sample - loss: 1.2322 - accuracy: 0.5228 - val_loss: 1.2720 - val_accuracy: 0.5143\n",
      "Epoch 36/100\n",
      "32298/32298 [==============================] - 6s 183us/sample - loss: 1.2353 - accuracy: 0.5250 - val_loss: 1.4011 - val_accuracy: 0.4695\n",
      "Epoch 37/100\n",
      "32298/32298 [==============================] - 6s 183us/sample - loss: 1.2311 - accuracy: 0.5232 - val_loss: 1.2729 - val_accuracy: 0.5104\n",
      "Epoch 38/100\n",
      "32298/32298 [==============================] - 6s 194us/sample - loss: 1.2327 - accuracy: 0.5238 - val_loss: 1.3273 - val_accuracy: 0.4909\n",
      "Epoch 39/100\n",
      "32298/32298 [==============================] - 6s 189us/sample - loss: 1.2324 - accuracy: 0.5231 - val_loss: 1.5185 - val_accuracy: 0.4140\n",
      "Epoch 40/100\n",
      "32298/32298 [==============================] - 6s 197us/sample - loss: 1.2337 - accuracy: 0.5223 - val_loss: 1.3297 - val_accuracy: 0.4887\n",
      "Epoch 41/100\n",
      "32298/32298 [==============================] - 7s 225us/sample - loss: 1.2328 - accuracy: 0.5228 - val_loss: 1.2902 - val_accuracy: 0.4971\n",
      "Epoch 42/100\n",
      "32298/32298 [==============================] - 6s 188us/sample - loss: 1.2339 - accuracy: 0.5258 - val_loss: 1.3704 - val_accuracy: 0.4648\n",
      "Epoch 43/100\n",
      "32298/32298 [==============================] - 6s 185us/sample - loss: 1.2326 - accuracy: 0.5253 - val_loss: 1.2692 - val_accuracy: 0.5132\n",
      "Epoch 44/100\n",
      "32298/32298 [==============================] - 6s 196us/sample - loss: 1.2309 - accuracy: 0.5283 - val_loss: 1.3174 - val_accuracy: 0.4951\n",
      "Epoch 45/100\n",
      "32298/32298 [==============================] - 8s 236us/sample - loss: 1.2320 - accuracy: 0.5235 - val_loss: 1.4733 - val_accuracy: 0.4126\n",
      "Epoch 46/100\n",
      "32298/32298 [==============================] - 6s 188us/sample - loss: 1.2291 - accuracy: 0.5240 - val_loss: 1.2860 - val_accuracy: 0.5043\n",
      "Epoch 47/100\n",
      "32298/32298 [==============================] - 6s 188us/sample - loss: 1.2309 - accuracy: 0.5242 - val_loss: 1.3629 - val_accuracy: 0.4862\n",
      "Epoch 48/100\n",
      "32298/32298 [==============================] - 8s 258us/sample - loss: 1.2278 - accuracy: 0.5246 - val_loss: 1.3373 - val_accuracy: 0.4918\n",
      "Epoch 49/100\n",
      "32298/32298 [==============================] - 8s 246us/sample - loss: 1.2339 - accuracy: 0.5233 - val_loss: 1.3689 - val_accuracy: 0.4759\n",
      "Epoch 50/100\n",
      "32298/32298 [==============================] - 9s 275us/sample - loss: 1.2279 - accuracy: 0.5246 - val_loss: 1.3188 - val_accuracy: 0.4923\n",
      "Epoch 51/100\n",
      "32298/32298 [==============================] - 7s 225us/sample - loss: 1.2296 - accuracy: 0.5242 - val_loss: 1.2885 - val_accuracy: 0.5093\n",
      "Epoch 52/100\n",
      "32298/32298 [==============================] - 7s 217us/sample - loss: 1.2329 - accuracy: 0.5208 - val_loss: 1.2790 - val_accuracy: 0.5079\n",
      "Epoch 53/100\n",
      "32298/32298 [==============================] - 7s 211us/sample - loss: 1.2254 - accuracy: 0.5254 - val_loss: 1.3170 - val_accuracy: 0.4960\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32298/32298 [==============================] - 7s 216us/sample - loss: 1.2200 - accuracy: 0.5287 - val_loss: 1.3201 - val_accuracy: 0.4887\n",
      "Epoch 55/100\n",
      "32298/32298 [==============================] - 9s 273us/sample - loss: 1.2255 - accuracy: 0.5256 - val_loss: 1.3428 - val_accuracy: 0.4812\n",
      "Epoch 56/100\n",
      "10304/32298 [========>.....................] - ETA: 4s - loss: 1.2224 - accuracy: 0.5285"
     ]
    }
   ],
   "source": [
    "model.fit(XDB,YDB,batch_size=64,epochs=100,validation_data=(XTB,YTB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49874616884926165"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(XTB).argmax(axis=1)==YTB.argmax(axis=1)).sum()/len(XTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 4, ..., 4, 4, 6])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(XTB).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 6, ..., 0, 3, 2])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YTB.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] 23 237 188 188\n",
      "[] 472 220 106 106\n",
      "[] 468 240 116 116\n",
      "[] 7 157 255 255\n",
      "[] 20 162 226 226\n",
      "[] 18 157 228 228\n",
      "[] 22 160 223 223\n",
      "[] 315 110 179 179\n",
      "[] 21 162 226 226\n",
      "[] 19 163 230 230\n",
      "[] 19 160 228 228\n",
      "[] 18 160 231 231\n",
      "[] 18 155 231 231\n",
      "[] 22 164 221 221\n",
      "[] 18 158 226 226\n",
      "[] 23 165 219 219\n",
      "[] 15 154 233 233\n",
      "[] 20 159 223 223\n",
      "[] 21 159 221 221\n",
      "[] 16 156 230 230\n",
      "[] 11 156 238 238\n",
      "[] 17 159 226 226\n",
      "[] 18 160 226 226\n",
      "[] 18 158 228 228\n",
      "[] 23 164 219 219\n",
      "[] 21 162 220 220\n",
      "[] 21 163 224 224\n",
      "[] 21 165 220 220\n",
      "[] 20 162 223 223\n",
      "[] 15 155 234 234\n",
      "[] 19 157 226 226\n",
      "[] 20 162 226 226\n",
      "[] 22 161 231 231\n",
      "[] 28 167 223 223\n",
      "[] 27 169 223 223\n",
      "[] 33 175 223 223\n",
      "[] 33 183 226 226\n",
      "[] 38 181 229 229\n",
      "[] 43 190 223 223\n",
      "[] 47 192 220 220\n",
      "[] 50 192 221 221\n",
      "[] 53 192 223 223\n",
      "[] 48 190 227 227\n",
      "[] 48 191 226 226\n",
      "[] 48 189 228 228\n",
      "[] 48 193 220 220\n",
      "[] 50 189 216 216\n",
      "[] 45 184 224 224\n",
      "[] 51 188 213 213\n",
      "[] 52 185 216 216\n",
      "[] 51 183 216 216\n",
      "[] 43 178 221 221\n",
      "[] 47 175 219 219\n",
      "[] 55 181 208 208\n",
      "[] 48 175 216 216\n",
      "[] 51 176 213 213\n",
      "[] 46 172 219 219\n",
      "[] 52 180 208 208\n",
      "[] 55 180 209 209\n",
      "[] 52 178 211 211\n",
      "[] 52 173 216 216\n",
      "[] 47 169 221 221\n",
      "[] 43 168 222 222\n",
      "[] 43 169 216 216\n",
      "[] 39 167 219 219\n",
      "[] 39 165 216 216\n",
      "[] 37 166 217 217\n",
      "[] 40 168 212 212\n",
      "[] 35 165 218 218\n",
      "[] 35 164 219 219\n",
      "[] 36 164 218 218\n",
      "[] 37 165 218 218\n",
      "[] 37 165 218 218\n",
      "[] 36 164 220 220\n",
      "[] 35 165 218 218\n",
      "[] 38 167 216 216\n",
      "[] 32 166 226 226\n",
      "[] 34 169 222 222\n",
      "[] 35 171 219 219\n",
      "[] 35 172 218 218\n",
      "[] 35 172 220 220\n",
      "[] 37 174 216 216\n",
      "[] 34 170 223 223\n",
      "[] 34 172 220 220\n",
      "[] 34 171 221 221\n",
      "[] 31 171 222 222\n",
      "[] 31 173 220 220\n",
      "[] 34 178 212 212\n",
      "[] 106 164 255 255\n",
      "[] 111 168 247 247\n",
      "[] 106 164 255 255\n",
      "[] 106 164 255 255\n",
      "[] 113 165 250 250\n",
      "[] 99 156 266 266\n",
      "[] 109 166 247 247\n",
      "[] 109 167 247 247\n",
      "[] 102 163 255 255\n",
      "[] 114 171 237 237\n",
      "[] 109 171 239 239\n",
      "[] 111 165 240 240\n",
      "[] 109 171 233 233\n",
      "[] 109 171 231 231\n",
      "[] 108 174 234 234\n",
      "[] 105 170 239 239\n",
      "[] 111 176 235 235\n",
      "[] 106 170 242 242\n",
      "[] 111 178 230 230\n",
      "[] 112 176 234 234\n",
      "[] 110 174 234 234\n",
      "[] 105 176 234 234\n",
      "[] 113 179 226 226\n",
      "[] 105 174 230 230\n",
      "[] 103 178 226 226\n",
      "[] 48 155 250 250\n",
      "[] 51 158 246 246\n",
      "[] 40 157 255 255\n",
      "[] 51 164 242 242\n",
      "[] 46 159 248 248\n",
      "[] 43 160 250 250\n",
      "[] 44 162 245 245\n",
      "[] 45 163 245 245\n",
      "[] 45 163 244 244\n",
      "[] 44 162 245 245\n",
      "[] 46 163 241 241\n",
      "[] 44 160 245 245\n",
      "[] 48 164 240 240\n",
      "[] 45 163 241 241\n",
      "[] 50 164 235 235\n",
      "[] 54 165 235 235\n",
      "[] 55 167 231 231\n",
      "[] 52 159 242 242\n",
      "[] 54 165 234 234\n",
      "[] 55 168 230 230\n",
      "[] 52 163 234 234\n",
      "[] 54 166 226 226\n",
      "[] 51 164 228 228\n",
      "[] 54 166 226 226\n",
      "[] 54 165 226 226\n",
      "[] 51 166 227 227\n",
      "[] 54 166 223 223\n",
      "[] 52 168 224 224\n",
      "[] 56 170 219 219\n",
      "[] 54 168 224 224\n",
      "[] 54 168 224 224\n",
      "[] 54 171 221 221\n",
      "[] 52 168 226 226\n",
      "[] 55 169 224 224\n",
      "[] 52 169 224 224\n",
      "[] 54 168 224 224\n",
      "[] 56 170 220 220\n",
      "[] 54 171 220 220\n",
      "[] 56 172 216 216\n",
      "[] 54 171 218 218\n",
      "[] 54 170 217 217\n",
      "[] 54 169 218 218\n",
      "[] 52 170 219 219\n",
      "[] 51 169 220 220\n",
      "[] 48 163 224 224\n",
      "[] 48 166 219 219\n",
      "[] 46 167 218 218\n",
      "[] 43 167 221 221\n",
      "[] 49 169 214 214\n",
      "[] 46 166 218 218\n",
      "[] 44 168 218 218\n",
      "[] 49 170 212 212\n",
      "[] 47 167 212 212\n",
      "[] 54 173 204 204\n",
      "[] 50 170 209 209\n",
      "[] 53 172 207 207\n",
      "[] 59 176 200 200\n",
      "[] 56 173 203 203\n",
      "[] 52 169 208 208\n",
      "[] 50 167 213 213\n",
      "[] 48 164 214 214\n",
      "[] 49 166 214 214\n",
      "[] 48 166 216 216\n",
      "[] 48 166 217 217\n",
      "[] 49 166 215 215\n",
      "[] 49 167 214 214\n",
      "[] 50 169 214 214\n",
      "[] 48 171 217 217\n",
      "[] 47 174 220 220\n",
      "[] 48 177 215 215\n",
      "[] 51 180 216 216\n",
      "[] 45 179 224 224\n",
      "[] 46 185 221 221\n",
      "[] 47 188 217 217\n",
      "[] 40 184 223 223\n",
      "[] 36 180 227 227\n",
      "[] 35 183 226 226\n",
      "[] 38 186 219 219\n",
      "[] 37 184 221 221\n",
      "[] 35 188 217 217\n",
      "[] 34 190 212 212\n",
      "[] 28 185 221 221\n",
      "[] 30 186 221 221\n",
      "[] 27 182 226 226\n",
      "[] 25 182 226 226\n",
      "[] 24 182 226 226\n",
      "[] 16 169 245 245\n",
      "[] 16 170 248 248\n",
      "[] 22 178 233 233\n",
      "[] 22 174 238 238\n",
      "[] 18 169 245 245\n",
      "[] 18 169 245 245\n",
      "[] 15 168 247 247\n",
      "[] 15 168 247 247\n",
      "[] 14 170 247 247\n",
      "[] 11 164 255 255\n",
      "[] 11 164 255 255\n",
      "[] 11 164 255 255\n",
      "[] 17 169 240 240\n",
      "[] 17 171 240 240\n",
      "[] 11 164 255 255\n",
      "[] 22 187 218 218\n",
      "[] 18 186 221 221\n",
      "[] 13 184 216 216\n",
      "[] 5 203 196 196\n",
      "[] 21 185 238 238\n",
      "[] 82 196 240 240\n",
      "[] 128 207 226 226\n",
      "[] 151 212 228 228\n",
      "[] 138 196 255 255\n",
      "[] 138 183 255 255\n",
      "[] 139 185 247 247\n",
      "[] 123 173 255 255\n",
      "[] 125 172 255 255\n",
      "[] 119 164 255 255\n",
      "[] 122 164 255 255\n",
      "[] 117 156 255 255\n",
      "[] 118 158 244 244\n",
      "[] 114 158 242 242\n",
      "[] 115 162 236 236\n",
      "[] 107 158 241 241\n",
      "[] 109 163 239 239\n",
      "[] 112 167 234 234\n",
      "[] 112 168 231 231\n",
      "[] 112 165 230 230\n",
      "[] 115 171 223 223\n",
      "[] 116 167 227 227\n",
      "[] 114 167 226 226\n",
      "[] 107 163 232 232\n",
      "[] 105 160 235 235\n",
      "[] 108 163 230 230\n",
      "[] 107 162 231 231\n",
      "[] 110 165 226 226\n",
      "[] 112 166 222 222\n",
      "[] 113 168 219 219\n",
      "[] 113 165 222 222\n",
      "[] 112 167 221 221\n",
      "[] 108 169 221 221\n",
      "[] 110 168 223 223\n",
      "[] 109 169 221 221\n",
      "[] 112 170 223 223\n",
      "[] 111 169 224 224\n",
      "[] 109 168 226 226\n",
      "[] 107 166 228 228\n",
      "[] 106 167 227 227\n",
      "[] 106 167 228 228\n",
      "[] 107 171 223 223\n",
      "[] 105 168 227 227\n",
      "[] 105 168 227 227\n",
      "[] 105 169 226 226\n",
      "[] 105 168 227 227\n",
      "[] 107 170 224 224\n",
      "[] 105 169 224 224\n",
      "[] 106 167 228 228\n",
      "[] 106 168 227 227\n",
      "[] 109 170 224 224\n",
      "[] 106 167 227 227\n",
      "[] 106 168 226 226\n",
      "[] 87 187 216 216\n",
      "[] 85 187 203 203\n",
      "[] 66 166 221 221\n",
      "[] 28 166 230 230\n",
      "[] 30 168 226 226\n",
      "[] 30 168 226 226\n",
      "[] 33 170 224 224\n",
      "[] 411 151 158 158\n",
      "[] 410 150 171 171\n",
      "[] 412 154 171 171\n",
      "[] 415 159 176 176\n",
      "[] 426 164 191 191\n",
      "[] 7 204 196 196\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cam.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    landmarks=[]\n",
    "    for (x,y,w,h) in faces:\n",
    "        svx,svy=x,y\n",
    "        gray=gray[y:y+h,x:x+w]\n",
    "        try:\n",
    "            gray=cv2.resize(gray, (face_dim, face_dim))\n",
    "        except:\n",
    "            print(gray,x,y,w,h)\n",
    "            continue\n",
    "        shape=predictor(gray,dlib.rectangle(0,0,face_dim,face_dim))\n",
    "        xlist=[]\n",
    "        ylist=[]\n",
    "        for i in range(68):\n",
    "            xp=shape.part(i).x\n",
    "            yp=shape.part(i).y\n",
    "            cv2.circle(gray, (xp, yp), 2, (255, 255, 255), -1)\n",
    "            xlist.append(float(xp))\n",
    "            ylist.append(float(yp))\n",
    "        cv2.imshow('gray',gray)\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        res=[]\n",
    "        for (x,y) in zip(xcentral,ycentral):\n",
    "            res.append(x)\n",
    "            res.append(y)\n",
    "        landmarks.append((np.asarray(res)/face_dim+1)/2)\n",
    "    if len(landmarks)>0:\n",
    "        y_out=model.predict(np.asarray(landmarks))\n",
    "        res=np.argmax(y_out,axis=1)\n",
    "        for r in res:\n",
    "            cv2.putText(img,emotions[r],(svx,svy),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),2,cv2.LINE_AA)\n",
    "    cv2.imshow('webcam', img)\n",
    "    if cv2.waitKey(1) & 0xff == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
